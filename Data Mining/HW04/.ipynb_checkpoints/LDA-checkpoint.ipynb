{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Linear Discriminant Analysis\n",
    "\n",
    "## Some theory\n",
    "\n",
    "<b>Goal:</b> we are in a two-class classification problem, therefore the goal is to predict the label $y \\in \\{0, 1\\}$ given a vector $x$ of $d$ features.\n",
    "\n",
    "Assumptions for this method:\n",
    "\n",
    "1. Both classes are equally likely, i.e. their prior probabilities $p(Y = 1)$ and $p(Y = 0)$ are equal.\n",
    "2. As a modelling assumption, we assume that our features or more precisely, their conditional probabiltiy $P(X \\mid Y)$, follow a multivariate normal distribution.\n",
    "3. Furthermore, we assume that their covariance matrices are equal: instead of working with $\\Sigma_0$ and $\\Sigma_1$, we thus only have to care about a *single* matrix $\\Sigma$. The two means $\\mu_0$, $\\mu_1$ may, instead, be different for each class.\n",
    "\n",
    "Given $y \\in \\{0,1\\}$, the probability density distribution for our features is thus given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu_y},\\Sigma) = \\frac{1}{(2 \\pi)^{\\frac{d}{2}} |\\Sigma|^{\\frac{1}{2}}} \\exp \\left( - \\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu_y})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_y)\\right) \n",
    "\\end{equation}\n",
    "\n",
    "<b>Aim:</b> finding out which class maximizes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\arg \\max_{y \\in \\{0, 1\\}} p(Y = y) p(X = \\mathbf{x}| Y = y)\n",
    "\\end{equation}\n",
    "\n",
    "In the lecture, you have seen that the basic idea involves predicting class $1$ if the log-likelihood ratio\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\mathcal{L}(\\mathbf{x}) = \\log \\left(\\frac{P(Y=1) P(X=\\mathbf{x} \\mid Y=1)}{P(Y=0) P(X=\\mathbf{x} \\mid Y=0)}\\right) > 0\n",
    "\\end{equation}\n",
    "\n",
    "or, more generally, greater than some positive threshold. If you look closely, you will see a resemblance to Bayes' theorem: the denominator and numerator are parts of the right-hand side of the theorem but the *evidence* term is ignored:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y \\mid X) = \\frac{P(X \\mid Y) P(Y)}{P(X)}\n",
    "\\end{equation}\n",
    "\n",
    "The evidence term can be somewhat hard to calculate, so the log-likelihood ratio is preferable because it is easier.\n",
    "\n",
    "Plugging in the normal distribution into the log likelihood ratio, we see that only the exponential terms remain because we assume that $\\Sigma_0 = \\Sigma_1 = \\Sigma$. Hence, we are left with an expression of the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log \\left(\\frac{\\exp \\left(-\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu}_1)^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1)\\right)}{\\exp \\left(-\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu}_0)^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_0)\\right)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\log\\left(\\frac{x}{y}\\right) = \\log(x) - \\log(y)$, and $\\log(\\exp(x)) = x$, the above equation simplifies to:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1)\n",
    "+\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_0)^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_0)\n",
    "\\end{equation}\n",
    "\n",
    "This expression can be brought into a simpler form by collecting terms that depend on $x$ and terms that do *not* depend on $x$. You saw this in the lecture and it yields a nice linear regression model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\mathbf{w}, \\mathbf{x} \\rangle + b \n",
    "\\end{equation}\n",
    "\n",
    "where: \n",
    "\\begin{equation}\n",
    "\\mathbf{w} = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T\\Sigma^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "and the offset is obtained as\n",
    "\\begin{equation}\n",
    " b = \\frac{1}{2}(\\boldsymbol{\\mu}_0^T \\Sigma^{-1} \\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_1^T \\Sigma^{-1} \\boldsymbol{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "All we need to do for performing LDA is to estimate $\\boldsymbol{\\mu}_0$, $\\boldsymbol{\\mu}_1$, and $\\Sigma$ from the data. Then, we calculate $b$ and $\\mathbf{w}$. Afterwards, we evaluate $\\langle \\mathbf{w}, \\mathbf{x}_{test} \\rangle + b $ and we check if it's greater than 0 or not, and classify $\\mathbf{x}_{test}$.\n",
    "\n",
    "However, as briefly mentioned on the slides, this classifier suffers from computational inefficiencies in higher dimensions, because inverting $\\Sigma$ is very costly. What's the complexity if $\\Sigma$ is $(d \\times d)$? \n",
    "\n",
    "$O(d^3)$. If $d$ is very high, this runtime is not properly manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "We will deal with a data set that records information about diabetic patients.\n",
    "\n",
    "Let's first load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('data/diabetes_train.csv')\n",
    "df_test = pd.read_csv('data/diabetes_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the data set...\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also make sure that the 'type' columns, which we want to use for\n",
    "# prediction, is really only a binary column.\n",
    "df_train['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having acquired some confidence, we finally obtain the matrices for a\n",
    "# subsequent prediction task.\n",
    "X_train = df_train.iloc[:, 0:7].values\n",
    "y_train = df_train['type'].values\n",
    "\n",
    "X_test = df_test.iloc[:, 0:7].values\n",
    "y_test = df_test['type'].values\n",
    "\n",
    "# Let's also check the shape, to be sure.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps follow the description of LDA from the course:\n",
    "\n",
    "1. We write a function for estimating the $\\mu$ vectors.\n",
    "2. We write a function to calculate the *inverse* of the sample covariance matrix $\\Sigma$.\n",
    "3. We write a function to compute the prediction based on the likelihood ratio test.\n",
    "3. We write a function for computing a vector $w$ and a scalar $b$ to mimic linear regression.\n",
    "4. We write a function for performing the prediction using $w$ and $b$.\n",
    "5. We start to alternate the way sentences in this list start.\n",
    "5. Profit!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mu(X, y, c):\n",
    "    '''\n",
    "    Performs mean estimation.\n",
    "    \n",
    "    :param X: Data matrix\n",
    "    :param y: Label vector\n",
    "    :param c: Selected class for which to estimate the mean\n",
    "    \n",
    "    :return: Mean estimate for the given class\n",
    "    '''\n",
    "    \n",
    "    # Some `numpy` goodness to make this easy: we first select all indices\n",
    "    # in the label vector, use *this* selection as a selection of the data\n",
    "    # matrix, and finally, calculate its mean.\n",
    "    #\n",
    "    # Setting `axis=0` means that we want means calculate over columns and\n",
    "    # not over rows (which contain our features).\n",
    "    return X[y == c].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's 'test' this real quick. In a proper scenario, we would be writing\n",
    "# unit tests right now.\n",
    "#\n",
    "# See https://docs.python.org/3/library/unittest.html for an introduction\n",
    "# or if you want to collect gold stars.\n",
    "estimate_mu(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without a proper test suite, I have no idea of knowing whether this is\n",
    "correct or not. But it does not contain any `NaN` or `inf` values, and\n",
    "I am willing to take this as a good sign.\n",
    "\n",
    "Onwards, with the inverse of the sample covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sigma_inv(X):\n",
    "    '''\n",
    "    Performs sample covariance estimation and inversion.\n",
    "    \n",
    "    :param X: Data matrix\n",
    "    \n",
    "    :return: Inverse of the sample covariance matrix\n",
    "    '''\n",
    "    \n",
    "    sigma = np.cov(X, rowvar=False)\n",
    "    return np.linalg.inv(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, let's 'test' this real quick. If you are shaking your head at the\n",
    "# fact that we solved this with `numpy`, we also have an implementation of\n",
    "# this that does *not* use `numpy`. But why walk when you can ride?\n",
    "estimate_sigma_inv(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we need to build the classifier using the log-likelihood test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use conventional LDA form for prediction\n",
    "def predict_likelihood_ratio(x, mu0, mu1, sigma_inv):\n",
    "    '''\n",
    "    This function serves for predicting the class label \n",
    "    only for one sample at a time\n",
    "    '''\n",
    "    class_1 = np.matmul(np.matmul((x - mu1), sigma_inv), (x - mu1).T)\n",
    "    class_0 = np.matmul(np.matmul((x - mu0), sigma_inv), (x - mu0).T)\n",
    "    \n",
    "    log_like_ratio = class_1 - class_0\n",
    "    return (log_like_ratio < 0).astype(float).reshape(-1)\n",
    "\n",
    "mu0 = estimate_mu(X_train, y_train, 0)\n",
    "mu1 = estimate_mu(X_train, y_train, 1)\n",
    "sigma_inv = estimate_sigma_inv(X_train)\n",
    "y_pred_likelihood_ratio = predict_likelihood_ratio(X_test[0], mu0, mu1, sigma_inv)\n",
    "y_pred_likelihood_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use conventional LDA form for prediction\n",
    "def predict_likelihood_ratio(x, mu0, mu1, sigma_inv):\n",
    "    '''\n",
    "    Predicting the class label given the entire dataset\n",
    "    '''\n",
    "    # Reshape the arrays such that we actually do an element-wise vector product\n",
    "    class_1 = np.matmul(((x - mu1).dot(sigma_inv)[:, np.newaxis,: ]), (x - mu1)[:, :, np.newaxis])\n",
    "    class_0 = np.matmul(((x - mu0).dot(sigma_inv)[:, np.newaxis,: ]), (x - mu0)[:, :, np.newaxis])\n",
    "    \n",
    "    log_like_ratio = class_1 - class_0\n",
    "    return (log_like_ratio < 0).astype(float).reshape(-1)\n",
    "\n",
    "\n",
    "# Here we are testing our implementation, not the LDA performance. Therefore, we could use\n",
    "# the training data for debugging our code. The evaluation performance is perfomed at the end\n",
    "# of this notebook.\n",
    "mu0 = estimate_mu(X_train, y_train, 0)\n",
    "mu1 = estimate_mu(X_train, y_train, 1)\n",
    "sigma_inv = estimate_sigma_inv(X_train)\n",
    "y_pred_likelihood_ratio_all = predict_likelihood_ratio(X_test, mu0, mu1, sigma_inv)\n",
    "y_pred_likelihood_ratio_all.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we now calculate the vectors $w$ and $b$ to prepare the prediction function. If you recall the lecture slides 'Link to linear regression', $w$ is calculated as a vector product of the difference in means and the inverse covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_w(mu0, mu1, sigma_inv):\n",
    "    '''\n",
    "    Calculates w vector that is required for predictions.\n",
    "    \n",
    "    :param mu0: Sample mean of the negative class\n",
    "    :param mu1: Sample mean of the positive class\n",
    "    :param sigma_inv: Inverse of the sample covariance matrix\n",
    "    \n",
    "    :return: Vector w\n",
    "    '''\n",
    "\n",
    "    # The dot product of `numpy` is phrased a little bit more generally than in some\n",
    "    # mathematical textbooks. It does nothing but vector--matrix multiplication here...\n",
    "    return (mu1 - mu0).dot(sigma_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small 'test' case. This is particularly useful to check whether we at least get an\n",
    "# array that has the proper shape.\n",
    "\n",
    "mu0 = estimate_mu(X_train, y_train, 0)\n",
    "mu1 = estimate_mu(X_train, y_train, 1)\n",
    "sigma_inv = estimate_sigma_inv(X_train)\n",
    "\n",
    "# We are feeling luck here...\n",
    "assert calculate_w(mu0, mu1, sigma_inv).shape[0] == X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_b(mu0, mu1, sigma_inv):\n",
    "    '''\n",
    "    Calculates scalar b that is required for predictions.\n",
    "    \n",
    "    :param mu0: Sample mean of the negative class\n",
    "    :param mu1: Sample mean of the positive class\n",
    "    :param sigma_inv: Inverse of the sample covariance matrix\n",
    "    \n",
    "    :return: Scalar b\n",
    "    '''\n",
    "       \n",
    "    # You can also do it without the transpose functionality, but this way looks\n",
    "    # a little bit more like on the slides.\n",
    "    return 0.5 * (mu0.dot(sigma_inv).dot(mu0.T) - mu1.dot(sigma_inv).dot(mu1.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And another 'test' case. This should be a scalar, otherwise something is seriously\n",
    "# wrong here.\n",
    "calculate_b(mu0, mu1, sigma_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we may now finally write the prediction function. Recall from the lecture that it uses the sign of the dot product to perform class prediction (this was motivated by showing that the log-likelihood ratio can be rewritten as a dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    '''\n",
    "    Performs LDA prediction of a whole data set matrix,\n",
    "    given the model parameters w and b.\n",
    "    \n",
    "    :param X: Data matrix\n",
    "    :param w: Vector w (calculated on training data)\n",
    "    :param b: Scalar b (calculated on training data)\n",
    "    \n",
    "    :return: Vector with predictions (0 or 1), for each\n",
    "    row of X.\n",
    "    '''\n",
    "    \n",
    "    # `numpy` will automatically 'broadcast' the dot operation here so that\n",
    "    # we obtain a vector of predictions. Likewise, b will be broadcast into\n",
    "    # a vector such that the addition operation works.\n",
    "    y_pred = np.sign(w.dot(X.T) + b)\n",
    "    y_pred[y_pred == -1] = 0\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this! Remember, we are testing the functions we wrote, not the goodness of our classifier.\n",
    "# Therefore it's not an error trying it using X_train.\n",
    "y_pred = predict(X_test, calculate_w(mu0, mu1, sigma_inv), calculate_b(mu0, mu1, sigma_inv))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the results of both approaches are in line\n",
    "assert np.allclose(y_pred, y_pred_likelihood_ratio_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of LDA's performance\n",
    "This looks fine so far, but now we want to evaluate this properly in terms of accuracy, precision & recall, etc., so we require an additional set of auxiliary functions. Let's start with the calculation of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    Given a set of true labels and predicted labels, calculates a\n",
    "    confusion matrix.\n",
    "    \n",
    "    :param y_true: True labels\n",
    "    :param y_pred: Predicted labels\n",
    "    \n",
    "    :return: Tuple of TP, TN, FP, FN\n",
    "    '''\n",
    "    \n",
    "    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n",
    "    TN = np.sum(np.logical_and(y_pred == 0, y_true == 0))\n",
    "    FP = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n",
    "    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n",
    "    \n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the confusion matrix information, we may now calculate all kinds of interesting quality measures to assess our predictions. A simple binary setting, accuracy plus precision and recall are probably sufficient. Implementing other measures could be beneficial for your understanding of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(TP, TN, FP, FN):\n",
    "    '''\n",
    "    Calculates the accuracy score.\n",
    "    \n",
    "    :param TP: Number of true positives\n",
    "    :param TN: Number of true negatives\n",
    "    :param FP: Number of false positives\n",
    "    :param FN: Number of false negatives\n",
    "    \n",
    "    :return: Accuracy score\n",
    "    '''\n",
    "    \n",
    "    return float(TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "\n",
    "# Precision answers the question 'Out of all positive predictions, what fraction\n",
    "# of them is actually correct?'\n",
    "def precision_score(TP, FP):\n",
    "    return float(TP) / (TP + FP)\n",
    "\n",
    "\n",
    "# Recall answers the question 'Out of all positive samples (!), what fraction is\n",
    "# correctly detected by our classifier?'\n",
    "def recall_score(TP, FN):\n",
    "    return float(TP) / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "The numbers above look quite okay, but they were calculated on the training data set, which is of course wrong. So let's put everything nicely together her and evaluate the classifier afterwards. The nice thing about this variant of LDA is that we do _not_ need any kind of hyperparameter training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = estimate_mu(X_train, y_train, 0)   # estimate mean on training data\n",
    "mu1 = estimate_mu(X_train, y_train, 1)   # ditto for other class\n",
    "sigma_inv = estimate_sigma_inv(X_train)  # estimate sample covariance matrix\n",
    "w = calculate_w(mu0, mu1, sigma_inv)     # calculate vector for prediction (ll-formulation)\n",
    "b = calculate_b(mu0, mu1, sigma_inv)     # calculate scalar for prediction (ll-formulation)\n",
    "\n",
    "# And now to predict on the *test* set. \n",
    "y_pred = predict(X_test, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('----------')\n",
    "print('{:>3} | {:>3}'.format(tp, fp))\n",
    "print('{:>3} | {:>3}'.format(fn, tn))\n",
    "print('----------\\n')\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(tp, tn, fp, fn)))\n",
    "print('Precision: {:.2f}'.format(precision_score(tp, fp)))\n",
    "print('Recall: {:.2f}'.format(recall_score(tp, fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing things\n",
    "\n",
    "In a proper evaluation setting, we are also interested in the general classifier performance over a wide variety of thresholds. This is particularly useful when the _prevalence_, i.e. the incidence of the positive class, is much lower and not balanced.\n",
    "\n",
    "To obtain a precision--recall curve with `scikit-learn`, we require a modified version of the prediction function that returns 'raw' scores instead of a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, w, b):\n",
    "    '''\n",
    "    Performs LDA prediction of a whole data set matrix,\n",
    "    given the model parameters w and b, but returns raw\n",
    "    scores instead of labels.\n",
    "    \n",
    "    :param X: Data matrix\n",
    "    :param w: Vector w (calculated on training data)\n",
    "    :param b: Scalar b (calculated on training data)\n",
    "    \n",
    "    :return: Vector with raw prediction scores for each\n",
    "    row of X.\n",
    "    '''\n",
    "    \n",
    "    y_pred_proba = w.dot(X.T) + b\n",
    "    return y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now use `precision_recall_curve` to generate a curve of all potential scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, predict_proba(X_test, w, b))\n",
    "auprc = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes().set_aspect('equal')\n",
    "plt.step(recall, precision, label='AUC: {:.2f}'.format(auprc))\n",
    "\n",
    "# To make this graphical representation really cool, we also calculate the\n",
    "# prevalence of the positive class in the test data. It will be shown as a\n",
    "# line in the plot. This line corresponds to the 'grumpy' classifier, i.e.\n",
    "# the classifier that assigns all samples the same class.\n",
    "\n",
    "prevalence = len(y_test[y_test == 1]) / len(y_test)\n",
    "\n",
    "plt.hlines(linestyle='--', alpha=0.5, y=prevalence, xmin=0, xmax=1, label='Random classifier')\n",
    "\n",
    "plt.title('LDA')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "### Computation of cov without numpy\n",
    "\n",
    "Here we briefly describe a way to calculate sample covariance matrices without heavy use of `numpy`. Read on if you are into that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov(X):\n",
    "    '''\n",
    "    Calculates a sample covariance matrix without a special `numpy` function.\n",
    "    \n",
    "    :param X: Data matrix\n",
    "    \n",
    "    :return: Sample covariance matrix\n",
    "    '''\n",
    "    \n",
    "    mu = np.array([X.mean(axis=0)])\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    sigma = np.zeros((num_features, num_features))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        residual = X[i, ] - mu\n",
    "        sigma += np.dot(residual.T, residual)\n",
    "\n",
    "    sigma /= (num_samples - 1)\n",
    "    \n",
    "    # This checks that we are not doing something incredibly\n",
    "    # wrong here.\n",
    "    assert np.allclose(sigma, np.cov(X, rowvar=False))\n",
    "    \n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
